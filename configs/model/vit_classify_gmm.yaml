_target_: src.models.module.GMMModule

net:
  _target_: src.models.components.nets.SingleEncMultiDecWithGMM
  spectrogram: 
    _target_: torchaudio.transforms.MelSpectrogram
    n_fft: 512
    n_mels: 80 
  encoder:
    _target_: src.models.components.encoder.vit.AudioDeiT
  decoders:
    classify:
      _target_: src.models.components.decoder.buncher_classifier.ClassifierBuncher
      classifier:
        _target_: torch.nn.Linear
        in_features: 960
        out_features: 5
      buncher:
        _target_: torch.nn.Sequential
        _args_:
          - _target_: src.models.components.layer.pooler.AvgPooler
          - _target_: src.models.components.layer.base.LambdaLayer
            lambda_func: "lambda x: x.squeeze(1).unsqueeze(-1).unsqueeze(-1)"
  gmm:
    _target_: torch.nn.Sequential
    _args_:
      - _target_: torch.nn.Dropout
        p: 0.25
      - _target_: torch.nn.Linear
        in_features: "${sum_all:
          ${....decoders.classify.classifier.in_features},
          ${....decoders.classify.classifier.out_features}
        }" # 192 + 5 + 2
        out_features: 32
      - _target_: torch.nn.PReLU
      - _target_: torch.nn.Linear
        in_features: ${..1.out_features}
        out_features: 32 # n_gmm
  decoder_post:
    _target_: src.models.components.nets.classify_decoder_post
    _partial_: True
gmm_loss:
  _target_: src.models.components.layer.loss.GMMLoss
  n_gmm: ${..net.gmm._args_.3.out_features}

losses:
  classify:
    ce:
      _target_: torch.nn.CrossEntropyLoss

losses_weights:
  classify:
    ce: 1.0
  gmm: 1.0

metrics:
  classify:
    acc:  
      _target_: torchmetrics.classification.MulticlassAccuracy
      num_classes: 5

optimizer:
  _target_: torch.optim.Adam
  _partial_: True
  lr: 0.0005
  weight_decay: 0.0

scheduler:
  _target_: cosine_annealing_warmup.CosineAnnealingWarmupRestarts
  _partial_: True
  # worse name, actually it is the number of epochs, equals torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.CosineAnnealingWarmRestarts.T_0
  first_cycle_steps: 10 
  # worse name, actually it is the number of epochs
  warmup_steps: 3
  # equals torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.CosineAnnealingWarmRestarts.T_mult
  cycle_mult: 2.0
  gamma: 0.5
  max_lr: ${..optimizer.lr}
  min_lr: 1e-6

scheduler_conf:
  # Metric to to monitor for schedulers like `ReduceLROnPlateau`
  monitor: "val/loss/total_epoch"
  # The unit of the scheduler's step size, could also be 'step'.
  # 'epoch' updates the scheduler on epoch end whereas 'step'
  # updates it after a optimizer update.
  interval": "epoch"
  # How many epochs/steps should pass between calls to
  # `scheduler.step()`. 1 corresponds to updating the learning
  # rate after every epoch/step.
  frequency: 1
  # If set to `True`, will enforce that the value specified 'monitor'
  # is available when the scheduler is updated, thus stopping
  # training if not found. If set to `False`, it will only produce a warning
  strict: True
